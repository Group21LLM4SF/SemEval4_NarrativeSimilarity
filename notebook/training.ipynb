{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b38f20",
   "metadata": {},
   "source": [
    "# Training Notebook - Narrative Similarity (SemEval4)\n",
    "\n",
    "This notebook implements the training of `AspectSupervisedEncoder` model for **narrative similarity** based on triplet learning with aspect attention. The model uses a pre-trained encoder (DistilBERT) with specialized projections to capture narrative similarity between texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa78e0",
   "metadata": {},
   "source": [
    "## 1. Path Setup\n",
    "\n",
    "The main directory is added to the system path to enable importing modules from the `src` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d44c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50189458",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "All necessary dependencies are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d736d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from src.trainer import Trainer\n",
    "from src.datasets.dataset import AspectTripletDatasetTrain, AspectTripletDatasetDev, load_aspect_triplets, load_eval_triplets\n",
    "from src.trainer import Trainer\n",
    "from src.models.encoder import AspectSupervisedEncoder\n",
    "import itertools\n",
    "import io\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc24de",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "\n",
    "The preprocessed data is loaded:\n",
    "- **Training data**: triplets (anchor, positive, negative) with augmentation based on narrative aspects\n",
    "- **Eval data**: triplets for model validation\n",
    "\n",
    "The files are in JSONL format and contain examples already processed for triplet learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab86340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 60)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_train = load_aspect_triplets(\"../data/processed/qwen_augment_from_partial_dev_large_with_aspects.jsonl\")\n",
    "llama_train = load_aspect_triplets(\"../data/processed/llama_augment_from_partial_dev_large_with_aspects.jsonl\")\n",
    "past_training_data = load_aspect_triplets(\"../data/processed/train_from_dev_w_aspect_aug_triplets.jsonl\")\n",
    "eval_data = load_eval_triplets(\"../data/processed/eval_from_dev_triplets.jsonl\")\n",
    "\n",
    "training_data = qwen_train + past_training_data + llama_train\n",
    "\n",
    "len(training_data), len(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d074bfd",
   "metadata": {},
   "source": [
    "## 4. Create DataLoaders\n",
    "\n",
    "The datasets and DataLoaders for training are created:\n",
    "- **AspectTripletDatasetTrain**: dataset for training with triplets (anchor, positive, negative)\n",
    "- **AspectTripletDatasetDev**: dataset for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AspectTripletDatasetTrain(training_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "eval_dataset = AspectTripletDatasetDev(eval_data)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f9500",
   "metadata": {},
   "source": [
    "## 5. Model Configuration\n",
    "\n",
    "The **AspectSupervisedEncoder** is initialized with the following parameters:\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `model_name` | distilbert-base-uncased | Pre-trained transformer model as base encoder |\n",
    "| `projection_dim` | 512 | Dimension of the projected embedding space |\n",
    "| `aspect_dim` | 128 | Dimension of aspect representations |\n",
    "| `num_heads` | 4 | Number of attention heads for multi-head attention |\n",
    "| `dropout` | 0.3 | Dropout probability for regularization |\n",
    "| `freeze_encoder` | True | Freeze pre-trained encoder weights |\n",
    "| `use_lora` | False | LoRA (Low-Rank Adaptation) disabled |\n",
    "\n",
    "**Note**: LoRA parameters (`lora_r=8`, `lora_alpha=16`) are configured but not active when `use_lora=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdddfaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "original_model = AspectSupervisedEncoder(\n",
    "    model_name=\"distilbert-base-uncased\", # You can change to other models\n",
    "    projection_dim=512,\n",
    "    aspect_dim=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    freeze_encoder=True,\n",
    ")\n",
    "\n",
    "# Count params\n",
    "total = sum(p.numel() for p in original_model.parameters())\n",
    "trainable = sum(p.numel() for p in original_model.parameters() if p.requires_grad)\n",
    "print(f\"Parameters: {trainable:,} trainable / {total:,} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a343d69",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "\n",
    "Training is configured and started.\n",
    "It automatically uses the GPU if available (CUDA), otherwise the CPU is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c132f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Hyperparameter Grid\n",
    "param_grid = {\n",
    "    'epochs': [5, 10],\n",
    "    'lr': [5e-4],\n",
    "    'weight_decay': [0.01, 0.001],\n",
    "    'triplet_margin': [0.3, 0.4, 0.5],\n",
    "    'cross_margin': [0.2],\n",
    "    'gamma': [0.3],       # Aspect loss weight\n",
    "    'beta': [1.0]         # Cross loss weight\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_grid_search(original_model, param_grid, train_loader, eval_loader):\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    best_concat_acc = 0.0\n",
    "    best_params = {}\n",
    "    best_metrics = {}\n",
    "    results = []\n",
    "\n",
    "    print(f\"Starting Grid Search with {len(combinations)} combinations...\\n\")\n",
    "\n",
    "    for i, params in enumerate(combinations):\n",
    "        print(f\"--- Run {i+1}/{len(combinations)}: {params} ---\")\n",
    "        \n",
    "        # [IMPORTANT] Deep Copy the model\n",
    "        current_model = copy.deepcopy(original_model)\n",
    "        current_model.to(DEVICE)\n",
    "        \n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=current_model, \n",
    "            device=DEVICE,\n",
    "            lr=params['lr'],\n",
    "            weight_decay=params['weight_decay'],\n",
    "            triplet_margin=params['triplet_margin'],\n",
    "            cross_margin=params['cross_margin'],\n",
    "            gamma=params['gamma'],\n",
    "            beta=params['beta']\n",
    "        )\n",
    "\n",
    "        # Capture Standard Output\n",
    "        captured_output = io.StringIO()\n",
    "        original_stdout = sys.stdout\n",
    "        \n",
    "        try:\n",
    "            sys.stdout = captured_output\n",
    "            trainer.fit(train_loader, eval_loader, epochs=params['epochs'])\n",
    "        finally:\n",
    "            sys.stdout = original_stdout\n",
    "\n",
    "        output_str = captured_output.getvalue()\n",
    "        \n",
    "        # 3. Parse Output for ALL Metrics\n",
    "        # Dictionary mapping your internal keys to the Regex patterns for the printed output\n",
    "        metric_patterns = {\n",
    "            \"Story\": r\"Story Embedding Only:\\s*(\\d+\\.\\d+)\",\n",
    "            \"Theme\": r\"Theme Aspect Only:\\s*(\\d+\\.\\d+)\",\n",
    "            \"Action\": r\"Action Aspect Only:\\s*(\\d+\\.\\d+)\",\n",
    "            \"Outcome\": r\"Outcome Aspect Only:\\s*(\\d+\\.\\d+)\",\n",
    "            \"Concatenated\": r\"Concatenated \\(Story \\+ Aspects\\):\\s*(\\d+\\.\\d+)\"\n",
    "        }\n",
    "        \n",
    "        current_metrics = {}\n",
    "        for key, pattern in metric_patterns.items():\n",
    "            match = re.search(pattern, output_str)\n",
    "            if match:\n",
    "                current_metrics[key] = float(match.group(1))\n",
    "            else:\n",
    "                current_metrics[key] = 0.0  # Default if parsing fails\n",
    "        \n",
    "        # Check if we successfully parsed the main metric\n",
    "        if current_metrics[\"Concatenated\"] > 0:\n",
    "            print(f\"Run {i+1} Result: {current_metrics}\")\n",
    "            \n",
    "            # Save params and full metrics dict to results\n",
    "            results.append((params, current_metrics))\n",
    "            \n",
    "            # Update Best Model (optimizing for Concatenated accuracy)\n",
    "            if current_metrics[\"Concatenated\"] > best_concat_acc:\n",
    "                best_concat_acc = current_metrics[\"Concatenated\"]\n",
    "                best_params = params\n",
    "                best_metrics = current_metrics\n",
    "        else:\n",
    "            print(f\"Run {i+1} Warning: Could not parse metrics. Check output format.\")\n",
    "\n",
    "    return best_params, best_metrics, results\n",
    "\n",
    "# Execute Grid Search\n",
    "# 'original_model' must be the variable name of your UNTRAINED model instance\n",
    "best_params, best_metrics, all_results = run_grid_search(original_model, param_grid, train_dataloader, eval_dataloader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"TOP PERFORMING CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Metrics:    {best_metrics}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
